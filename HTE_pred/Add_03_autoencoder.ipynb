{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try autoencoder SMILES and use the latent space to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# if use 19_science data\n",
    "# df = pd.read_csv(\"data/19_science/19_science_total.csv\")\n",
    "# df[\"full_smi\"] = df[\"Catalyst_smi\"] + \".\" + df[\"Imine_smi\"] + \".\" + df[\"Thiol_smi\"]\n",
    "\n",
    "# if use 18_science data\n",
    "# df = pd.read_excel('data/18_science/18 science_original_chem.xlsx')\n",
    "# df[\"full_smi\"] = df[\"Ligand\"] + \".\" + df[\"Additive\"] + \".\" + df[\"Base\"]+\".\" + df[\"Aryl halide\"]\n",
    "\n",
    "# if use suzuki data\n",
    "# df = pd.read_excel('data/suzuki/suzuki.xlsx').dropna().reset_index(drop=True)\n",
    "# df[\"full_smi\"] = df[\"reactant_1\"] + \".\" + df[\"reactant_2\"] + \".\" + df[\"ligand\"]+\".\" + df[\"reagent_1\"]+\".\" + df[\"solvent_1\"]\n",
    "\n",
    "# if use conjugate data\n",
    "df = pd.read_csv('data/peptide_data/conjugate_addition.csv').dropna().reset_index(drop=True)\n",
    "df[\"full_smi\"] = df[\"peptide_SMILES\"] + \".\" + df[\"Reactant_1_smi\"] + \".\" + df[\"Reactant_2_smi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 构建字符索引\n",
    "all_chars = set(\"\".join(df[\"full_smi\"].tolist()))\n",
    "char_to_index = {char: idx for idx, char in enumerate(sorted(all_chars))}\n",
    "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
    "\n",
    "MAX_LEN = max(df[\"full_smi\"].apply(len))\n",
    "NCHARS = len(char_to_index)\n",
    "\n",
    "# one-hot 编码函数\n",
    "def smiles_to_onehot(smiles, max_len=MAX_LEN, n_chars=NCHARS):\n",
    "    onehot = np.zeros((max_len, n_chars), dtype=np.float32)\n",
    "    for i, c in enumerate(smiles[:max_len]):\n",
    "        if c in char_to_index:\n",
    "            onehot[i, char_to_index[c]] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, conv_channels=64, latent_dim=128):\n",
    "        super(SMILESEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, conv_channels, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(conv_channels, conv_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(conv_channels * MAX_LEN, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, n_chars)\n",
    "        x = x.permute(0, 2, 1)  # 转为 (batch, channels, seq_len)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        z = self.fc2(x)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import time\n",
    "\n",
    "def evaluate_model(X, y, model_type=\"MLP\", n_runs=30):\n",
    "    r2_scores, maes, times = [], [], []\n",
    "\n",
    "    for _ in tqdm(range(n_runs)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "        if model_type == \"MLP\":\n",
    "            model = MLPRegressor(hidden_layer_sizes=(256, 128), activation='relu',\n",
    "                                 solver='adam', alpha=1e-4, max_iter=1000, random_state=0, early_stopping=False)\n",
    "        elif model_type == \"RF\":\n",
    "            model = RandomForestRegressor(n_estimators=300,max_depth=15, random_state=42, n_jobs=-1) # , min_samples_split=5\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        maes.append(mean_absolute_error(y_test, y_pred))\n",
    "        times.append(duration)\n",
    "        \n",
    "    return {\n",
    "        'R2_mean': np.mean(r2_scores),\n",
    "        'R2_std': np.std(r2_scores),\n",
    "        'MAE_mean': np.mean(maes),\n",
    "        'MAE_std': np.std(maes),\n",
    "        'Time_mean': np.mean(times),\n",
    "        'Time_std': np.std(times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:27<00:00,  1.08it/s]\n",
      "100%|██████████| 30/30 [00:03<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature_Type Model  Dim   R2_mean    R2_std   MAE_mean   MAE_std\n",
      "0  Autoencoder    RF  128  0.254873  0.074901  52.698247  3.144060\n",
      "1  Autoencoder   MLP  128 -0.027859  0.039978  66.564334  1.976207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 编码所有 SMILES\n",
    "onehot_encoded = np.stack([smiles_to_onehot(smi) for smi in df[\"full_smi\"]])\n",
    "input_tensor = torch.tensor(onehot_encoded)\n",
    "\n",
    "# 构建并编码\n",
    "encoder = SMILESEncoder(input_dim=NCHARS, latent_dim=128)\n",
    "with torch.no_grad():\n",
    "    latent_vectors = encoder(input_tensor)\n",
    "\n",
    "# 保存 latent vectors 和产率\n",
    "X_latent = latent_vectors.numpy()\n",
    "y_output = df[\"Reaction I eesyn (%)\"].values\n",
    "\n",
    "results = []\n",
    "\n",
    "for X, name, dim in [\n",
    "    # (X_onehot, \"OneHot_SMILES\", X_onehot.shape[1]),\n",
    "    (X_latent, \"Autoencoder\", X_latent.shape[1]),\n",
    "]:\n",
    "    for model in [\"RF\",\"MLP\"]:\n",
    "        result = evaluate_model(X, y_output, model_type=model, n_runs=30)\n",
    "        results.append([name, model, dim,\n",
    "                        result['R2_mean'], result['R2_std'],\n",
    "                        result['MAE_mean'], result['MAE_std']])\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[\n",
    "    \"Feature_Type\", \"Model\", \"Dim\",\n",
    "    \"R2_mean\", \"R2_std\", \"MAE_mean\", \"MAE_std\"\n",
    "])\n",
    "print(result_df)\n",
    "result_df.to_csv('results/ADD/conjugate_Autoencoder.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/peptide_data/'\n",
    "# OUT_DIR = 'out/models_unimol_infer'+datetime.now().strftime('%y%m%d%H%M')+'/'\n",
    "OUT_DIR = 'out/'\n",
    "\n",
    "INPUTS_tripep_repr = 'peptide_SMILES.csv'  # Unscaled  data \n",
    "INPUTS_reactant1_repr = 'Reactant_1_smi.csv'\n",
    "INPUTS_reactant2_repr = 'Reactant_2_smi.csv'\n",
    "INPUTS_Origin_DF = 'conjugate_addition.csv'\n",
    "\n",
    "inputs_tripep_repr = pd.read_csv(DATA_DIR + INPUTS_tripep_repr)\n",
    "inputs_reactant1_repr = pd.read_csv(DATA_DIR + INPUTS_reactant1_repr)\n",
    "inputs_reactant2_repr = pd.read_csv(DATA_DIR + INPUTS_reactant2_repr)\n",
    "\n",
    "yields = pd.read_csv(DATA_DIR + INPUTS_Origin_DF)['Reaction I eesyn (%)']\n",
    "\n",
    "inputs = np.concatenate([inputs_tripep_repr ,inputs_reactant1_repr,inputs_reactant2_repr],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:20<00:00,  2.67s/it]\n",
      "100%|██████████| 30/30 [02:27<00:00,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature_Type Model   Dim   R2_mean    R2_std   MAE_mean   MAE_std\n",
      "0       unimol    RF  1536  0.826865  0.080260  21.005526  3.579010\n",
      "1       unimol   MLP  1536  0.634688  0.133655  31.171844  5.432956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "X = inputs\n",
    "\n",
    "for X, name, dim in [\n",
    "    # (X_onehot, \"OneHot_SMILES\", X_onehot.shape[1]),\n",
    "    (inputs, \"unimol\", inputs.shape[1]),\n",
    "]:\n",
    "    for model in [\"RF\",\"MLP\"]:\n",
    "        result = evaluate_model(X, yields, model_type=model, n_runs=30)\n",
    "        results.append([name, model, dim,\n",
    "                        result['R2_mean'], result['R2_std'],\n",
    "                        result['MAE_mean'], result['MAE_std']])\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[\n",
    "    \"Feature_Type\", \"Model\", \"Dim\",\n",
    "    \"R2_mean\", \"R2_std\", \"MAE_mean\", \"MAE_std\"\n",
    "])\n",
    "print(result_df)\n",
    "result_df.to_csv('results/ADD/peptide_unimol_70_30.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照多肽种类划分 没用，文章可能是有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R2_mean': -0.037125025195120065, 'R2_std': 0.0, 'MAE_mean': 64.12308333333331, 'MAE_std': 1.4210854715202004e-14, 'Time_mean': 3.0255325555801393, 'Time_std': 0.040060729718162005}\n",
      "  Feature_Type Model   Dim   R2_mean        R2_std   MAE_mean       MAE_std\n",
      "0       unimol    RF  1536  0.850623  7.129368e-02  18.994637  3.418826e+00\n",
      "1       unimol   MLP  1536  0.691957  1.000335e-01  29.409517  4.701280e+00\n",
      "2       unimol    RF  1536 -0.037125  0.000000e+00  64.123083  1.421085e-14\n",
      "3       unimol   MLP  1536 -0.736334  1.110223e-16  79.703945  0.000000e+00\n",
      "4       unimol    RF  1536 -0.037125  0.000000e+00  64.123083  1.421085e-14\n",
      "5       unimol   MLP  1536 -0.736334  1.110223e-16  79.703945  0.000000e+00\n",
      "6       unimol    RF  1536 -0.037125  0.000000e+00  64.123083  1.421085e-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"./data/peptide_data/conjugate_addition.csv\")\n",
    "\n",
    "DATA_DIR = './data/peptide_data/'\n",
    "# OUT_DIR = 'out/models_unimol_infer'+datetime.now().strftime('%y%m%d%H%M')+'/'\n",
    "OUT_DIR = 'out/'\n",
    "\n",
    "INPUTS_tripep_repr = 'peptide_SMILES.csv'  # Unscaled  data \n",
    "INPUTS_reactant1_repr = 'Reactant_1_smi.csv'\n",
    "INPUTS_reactant2_repr = 'Reactant_2_smi.csv'\n",
    "INPUTS_Origin_DF = 'conjugate_addition.csv'\n",
    "\n",
    "inputs_tripep_repr = pd.read_csv(DATA_DIR + INPUTS_tripep_repr)\n",
    "inputs_reactant1_repr = pd.read_csv(DATA_DIR + INPUTS_reactant1_repr)\n",
    "inputs_reactant2_repr = pd.read_csv(DATA_DIR + INPUTS_reactant2_repr)\n",
    "\n",
    "yields = pd.read_csv(DATA_DIR + INPUTS_Origin_DF)['Reaction I eesyn (%)']\n",
    "\n",
    "inputs = np.concatenate([inputs_tripep_repr ,inputs_reactant1_repr,inputs_reactant2_repr],axis=1)\n",
    "\n",
    "# 设置随机种子（可选）\n",
    "random.seed(42)\n",
    "\n",
    "peptide_list = df['Peptide'].unique().tolist()\n",
    "peptide_list = [e for e in peptide_list if e != \"P5\"]\n",
    "selected_9 = random.sample(peptide_list, 9)\n",
    "final_selection = [\"P5\"] + selected_9\n",
    "\n",
    "test_idx = df[df[\"Peptide\"].isin(final_selection)].index\n",
    "\n",
    "X_test = inputs[test_idx]\n",
    "X_train = np.delete(inputs, test_idx, axis=0)\n",
    "\n",
    "y = yields  # 如果有标签列的话\n",
    "y_test = y[test_idx]\n",
    "y_train = np.delete(y, test_idx, axis=0)\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in [\"RF\"]:\n",
    "    result = evaluate_model(X_train, y_train, X_test,y_test, model_type=model, n_runs=10)\n",
    "    print(result)\n",
    "    results.append([name, model, dim,\n",
    "                    result['R2_mean'], result['R2_std'],\n",
    "                    result['MAE_mean'], result['MAE_std']])\n",
    "result_df = pd.DataFrame(results, columns=[\n",
    "    \"Feature_Type\", \"Model\", \"Dim\",\n",
    "    \"R2_mean\", \"R2_std\", \"MAE_mean\", \"MAE_std\"\n",
    "])\n",
    "print(result_df)\n",
    "result_df.to_csv('results/ADD/peptide_sequence_split_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 55)\n"
     ]
    }
   ],
   "source": [
    "smi_columns = ['peptide_SMILES', 'Reactant_1_smi', 'Reactant_2_smi']\n",
    "\n",
    "# 对每个分子列进行 One-Hot 编码\n",
    "encoded_parts = []\n",
    "for col in smi_columns:\n",
    "    onehot = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "    encoded_parts.append(onehot)\n",
    "\n",
    "# 合并编码后的部分和输出\n",
    "encoded_df = pd.concat(encoded_parts, axis=1)\n",
    "\n",
    "# 存储为新表格\n",
    "encoded_df.to_csv('HTE_descriptors/OH_encode/pep_onehot.csv', index=False)\n",
    "# print(encoded_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peptide_SMILES_MACCS.csv的维度: (200, 167)\n",
      "Reactant_1_smi_MACCS.csv的维度: (200, 167)\n",
      "Reactant_2_smi_MACCS.csv的维度: (200, 167)\n",
      "合并后的维度: (200, 501)\n"
     ]
    }
   ],
   "source": [
    "des_path = 'HTE_descriptors'\n",
    "\n",
    "def read_des(folder='folder_name'):\n",
    "    # folder = 'MFP_descriptor'\n",
    "    file_list = [f for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    # 读取并处理每个文件\n",
    "    fp_dfs = []\n",
    "    for filename in file_list:\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        if 'Original_SMILES' in df.columns:\n",
    "            df = df.drop(columns=['Original_SMILES'])\n",
    "    \n",
    "        # 使用文件名作为前缀（去掉扩展名）\n",
    "        prefix = os.path.splitext(filename)[0]\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "        print(f\"{filename}的维度:\", df.shape)\n",
    "        fp_dfs.append(df)\n",
    "\n",
    "    # 合并所有分子指纹特征\n",
    "    combined_fp = pd.concat(fp_dfs, axis=1)\n",
    "    # 输出结果查看\n",
    "    print(\"合并后的维度:\", combined_fp.shape)\n",
    "    return combined_fp\n",
    "\n",
    "des_path = 'HTE_descriptors'\n",
    "RDkit_pep = read_des(os.path.join(des_path, 'MACCS/peptide'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, y_train,X_test,y_test, model_type=\"MLP\", n_runs=30):\n",
    "    r2_scores, maes, times = [], [], []\n",
    "\n",
    "    for _ in tqdm(range(n_runs)):\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "        if model_type == \"MLP\":\n",
    "            model = MLPRegressor(hidden_layer_sizes=(256, 128), activation='relu',\n",
    "                                 solver='adam', alpha=1e-4, max_iter=1000, random_state=0)\n",
    "        elif model_type == \"RF\":\n",
    "            model = RandomForestRegressor(n_estimators=300,max_depth=15, random_state=42, n_jobs=-1) # , min_samples_split=5\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model.\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        maes.append(mean_absolute_error(y_test, y_pred))\n",
    "        times.append(duration)\n",
    "\n",
    "        # model.fit(X_train, y_train)\n",
    "        # y_pred = model.predict(X_test)\n",
    "        # r2_scores.append(r2_score(y_test, y_pred))\n",
    "        # maes.append(mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        'R2_mean': np.mean(r2_scores),\n",
    "        'R2_std': np.std(r2_scores),\n",
    "        'MAE_mean': np.mean(maes),\n",
    "        'MAE_std': np.std(maes),\n",
    "        'Time_mean': np.mean(times),\n",
    "        'Time_std': np.std(times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P9', 'P41', 'P33', 'P32', 'P25', 'P24', 'P22', 'P20', 'P2', 'P5']\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "DATA_DIR = './data/peptide_data/'\n",
    "# OUT_DIR = 'out/models_unimol_infer'+datetime.now().strftime('%y%m%d%H%M')+'/'\n",
    "OUT_DIR = 'out/peptide/'\n",
    "\n",
    "INPUTS_tripep_repr = 'peptide_SMILES.csv'  # Unscaled  data \n",
    "INPUTS_reactant1_repr = 'Reactant_1_smi.csv'\n",
    "INPUTS_reactant2_repr = 'Reactant_2_smi.csv'\n",
    "INPUTS_Origin_DF = 'conjugate_addition.csv'\n",
    "\n",
    "inputs_tripep_repr = pd.read_csv(DATA_DIR + INPUTS_tripep_repr)\n",
    "inputs_reactant1_repr = pd.read_csv(DATA_DIR + INPUTS_reactant1_repr)\n",
    "inputs_reactant2_repr = pd.read_csv(DATA_DIR + INPUTS_reactant2_repr)\n",
    "\n",
    "yields = pd.read_csv(DATA_DIR + INPUTS_Origin_DF)['Reaction I eesyn (%)']\n",
    "\n",
    "\n",
    "inputs = np.concatenate([encoded_df]) # np.concatenate([encoded_df])\n",
    "# inputs = np.concatenate([inputs_tripep_repr ,inputs_reactant1_repr,inputs_reactant2_repr],axis=1)\n",
    "\n",
    "total_cleaned_len = len(inputs_tripep_repr)\n",
    "\n",
    "# # pca\n",
    "# pca = PCA(n_components=100)\n",
    "# inputs_reduced = pca.fit_transform(inputs)\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "peptide_list = df['Peptide'].unique().tolist()\n",
    "# peptide_list = [e for e in peptide_list if e != \"P5\"]\n",
    "selected_9 = random.sample(peptide_list, 10)\n",
    "\n",
    "# final_selection =  selected_9 # [\"P5\"] +\n",
    "final_selection = ['P9', 'P41', 'P33', 'P32', 'P25', 'P24', 'P22', 'P20', 'P2', 'P5'] #['P9', 'P27', 'P15', 'P16', 'P14', 'P12', 'P11', 'P11', 'P1', 'P5']\n",
    "\n",
    "print(final_selection)\n",
    "test_idx = df[df[\"Peptide\"].isin(final_selection)].index.to_list()\n",
    "\n",
    "test_indices = np.zeros((total_cleaned_len), dtype=bool)\n",
    "test_indices[test_idx] = True  # 只在 test_idx 的位置上设为 True\n",
    "\n",
    "# 训练集布尔索引是取反\n",
    "train_indices = ~test_indices\n",
    "\n",
    "# 保存索引文件\n",
    "np.savetxt(OUT_DIR + 'clean_data_train_indices.csv', train_indices, delimiter=',')\n",
    "np.savetxt(OUT_DIR + 'clean_data_test_indices.csv', test_indices, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(inputs):  200\n",
      "batch_size:  160\n",
      "batch_size_test:  40\n"
     ]
    }
   ],
   "source": [
    "# Load yield data\n",
    "ee = np.array(yields)\n",
    "ee = ee.flatten()\n",
    "ee = np.nan_to_num(ee, nan=0)\n",
    "print('len(inputs): ', len(inputs))\n",
    "# Use the indices to generate train/test sets\n",
    "X_train = inputs[train_indices]\n",
    "#X_train = inputs[:2]\n",
    "y_train = ee[train_indices]\n",
    "featuresTrain = torch.from_numpy(X_train)\n",
    "targetsTrain = torch.from_numpy(y_train)\n",
    "batch_size = len(X_train)\n",
    "print('batch_size: ', batch_size)\n",
    "\n",
    "X_test = inputs[test_indices]\n",
    "y_test = ee[test_indices]\n",
    "featuresTest = torch.from_numpy(X_test)\n",
    "targetsTest = torch.from_numpy(y_test)#.type(torch.LongTensor)\n",
    "batch_size_test = len(X_test)\n",
    "print('batch_size_test: ', batch_size_test)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size_test, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R2_mean': -0.5399905718486462, 'R2_std': 0.0, 'MAE_mean': 27.863472672242473, 'MAE_std': 0.0, 'Time_mean': 0.26892805099487305, 'Time_std': 0.0}\n",
      "  Feature_Type Model   Dim   R2_mean  R2_std   MAE_mean  MAE_std\n",
      "0       unimol    RF  1536 -0.539991     0.0  27.863473      0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for model in [\"RF\"]:\n",
    "    result = evaluate_model(X_train, y_train, X_test,y_test, model_type=model, n_runs=1)\n",
    "    print(result)\n",
    "    results.append([name, model, dim,\n",
    "                    result['R2_mean'], result['R2_std'],\n",
    "                    result['MAE_mean'], result['MAE_std']])\n",
    "result_df = pd.DataFrame(results, columns=[\n",
    "    \"Feature_Type\", \"Model\", \"Dim\",\n",
    "    \"R2_mean\", \"R2_std\", \"MAE_mean\", \"MAE_std\"\n",
    "])\n",
    "print(result_df)\n",
    "result_df.to_csv('results/ADD/peptide_sequence_split_new.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unimol_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
