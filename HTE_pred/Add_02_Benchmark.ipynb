{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE19:(1075, 53), OHE18:(3955, 44), OHE_sm:(4620, 33)\n",
      "Catalyst_smi_MACCS.csv的维度: (1075, 167)\n",
      "Imine_smi_MACCS.csv的维度: (1075, 167)\n",
      "MACCS.csv的维度: (5, 167)\n",
      "Thiol_smi_MACCS.csv的维度: (1075, 167)\n",
      "合并后的维度: (1075, 668)\n",
      "Additive_MACCS.csv的维度: (3955, 167)\n",
      "Aryl halide_MACCS.csv的维度: (3955, 167)\n",
      "Base_MACCS.csv的维度: (3955, 167)\n",
      "Ligand_MACCS.csv的维度: (3955, 167)\n",
      "合并后的维度: (3955, 668)\n",
      "ligand_fingerprints.csv的维度: (4620, 167)\n",
      "reactant_1_fingerprints.csv的维度: (4620, 167)\n",
      "reactant_2_fingerprints.csv的维度: (4620, 167)\n",
      "reagent_1_fingerprints.csv的维度: (4620, 167)\n",
      "solvent_1_fingerprints.csv的维度: (4620, 167)\n",
      "合并后的维度: (4620, 835)\n",
      "Catalyst_smi_descriptors.csv的维度: (1075, 209)\n",
      "Imine_smi_descriptors.csv的维度: (1075, 209)\n",
      "Thiol_smi_descriptors.csv的维度: (1075, 209)\n",
      "合并后的维度: (1075, 627)\n",
      "Additive_descriptors.csv的维度: (3955, 209)\n",
      "Aryl halide_descriptors.csv的维度: (3955, 209)\n",
      "Base_descriptors.csv的维度: (3955, 209)\n",
      "Ligand_descriptors.csv的维度: (3955, 209)\n",
      "合并后的维度: (3955, 836)\n",
      "ligand_descriptors.csv的维度: (4620, 209)\n",
      "reactant_1_descriptors.csv的维度: (4620, 209)\n",
      "reactant_2_descriptors.csv的维度: (4620, 209)\n",
      "reagent_1_descriptors.csv的维度: (4620, 209)\n",
      "solvent_1_descriptors.csv的维度: (4620, 209)\n",
      "合并后的维度: (4620, 1045)\n",
      "Catalyst_1421.csv的维度: (1075, 1421)\n",
      "Imine_1435.csv的维度: (1075, 1435)\n",
      "Thiol_(1075, 1223).csv的维度: (1075, 1223)\n",
      "合并后的维度: (1075, 4079)\n",
      "Additive_(3955, 1282).csv的维度: (3955, 1282)\n",
      "Aryl halide_(3955, 1282).csv的维度: (3955, 1282)\n",
      "Base_(3955, 1377).csv的维度: (3955, 1377)\n",
      "Ligand_(3955, 1436).csv的维度: (3955, 1436)\n",
      "合并后的维度: (3955, 5377)\n",
      "Ligand_(4620, 1065).csv的维度: (4620, 1065)\n",
      "reactant_1_(4620, 1164).csv的维度: (4620, 1164)\n",
      "reactant_2_(4620, 1177).csv的维度: (4620, 1177)\n",
      "reagent_1_(4620, 783).csv的维度: (4620, 783)\n",
      "solvent_1_(4620, 927).csv的维度: (4620, 927)\n",
      "合并后的维度: (4620, 5116)\n",
      "Catalyst_smirepr.csv的维度: (1075, 512)\n",
      "Imine_smirepr.csv的维度: (1075, 512)\n",
      "Thiol_smirepr.csv的维度: (1075, 512)\n",
      "合并后的维度: (1075, 1536)\n",
      "Additiverepr.csv的维度: (3955, 512)\n",
      "Aryl haliderepr.csv的维度: (3955, 512)\n",
      "Baserepr.csv的维度: (3955, 512)\n",
      "Ligandrepr.csv的维度: (3955, 512)\n",
      "合并后的维度: (3955, 2048)\n",
      "ligandrepr.csv的维度: (4620, 512)\n",
      "reactant_1repr.csv的维度: (4620, 512)\n",
      "reactant_2repr.csv的维度: (4620, 512)\n",
      "reagent_1repr.csv的维度: (4620, 512)\n",
      "solvent_1repr.csv的维度: (4620, 512)\n",
      "合并后的维度: (4620, 2560)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "df_19 = pd.read_csv('data/19_science/19_science_sorted.csv', )\n",
    "df_18 = pd.read_excel('data/18_science/18 science_original_chem.xlsx')\n",
    "df_sm = pd.read_excel(\"data/suzuki/suzuki.xlsx\").dropna().reset_index(drop=True)\n",
    "\n",
    "# read descriptor\n",
    "des_path = 'HTE_descriptors'\n",
    "\n",
    "def read_des(folder='folder_name'):\n",
    "    # folder = 'MFP_descriptor'\n",
    "    file_list = [f for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    # 读取并处理每个文件\n",
    "    fp_dfs = []\n",
    "    for filename in file_list:\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        if 'Original_SMILES' in df.columns:\n",
    "            df = df.drop(columns=['Original_SMILES'])\n",
    "    \n",
    "        # 使用文件名作为前缀（去掉扩展名）\n",
    "        prefix = os.path.splitext(filename)[0]\n",
    "        df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "        print(f\"{filename}的维度:\", df.shape)\n",
    "        fp_dfs.append(df)\n",
    "\n",
    "    # 合并所有分子指纹特征\n",
    "    combined_fp = pd.concat(fp_dfs, axis=1)\n",
    "    # 输出结果查看\n",
    "    print(\"合并后的维度:\", combined_fp.shape)\n",
    "    return combined_fp\n",
    "\n",
    "# one-hot encode\n",
    "OHE_19 = pd.read_csv(os.path.join(des_path, f\"OH_encode/19_onehot.csv\"))\n",
    "OHE_18 = pd.read_csv(os.path.join(des_path, f\"OH_encode/18_onehot.csv\"))\n",
    "OHE_sm = pd.read_csv(os.path.join(des_path, f\"OH_encode/sm_onehot.csv\"))\n",
    "\n",
    "print(f\"OHE19:{OHE_19.shape}, OHE18:{OHE_18.shape}, OHE_sm:{OHE_sm.shape}\")\n",
    "\n",
    "# 分子指纹（Morgan Fingerprint）\n",
    "\n",
    "# FP_19 = read_des(os.path.join(des_path, 'MFP_descriptor/19_data'))\n",
    "# FP_18 = read_des(os.path.join(des_path, 'MFP_descriptor/18_data'))\n",
    "# FP_sm = read_des(os.path.join(des_path, 'MFP_descriptor/suzuki'))\n",
    "FP_19 = read_des(os.path.join(des_path, 'MACCS/19_data'))\n",
    "FP_18 = read_des(os.path.join(des_path, 'MACCS/18_data'))\n",
    "FP_sm = read_des(os.path.join(des_path, 'MACCS/suzuki'))\n",
    "\n",
    "# RDKit 分子性质\n",
    "RDkit_19 = read_des(os.path.join(des_path, 'RDKit_descriptors/19_data'))\n",
    "RDKit_18 = read_des(os.path.join(des_path, 'RDKit_descriptors/18_data'))\n",
    "RDkit_sm = read_des(os.path.join(des_path, 'RDKit_descriptors/suzuki'))\n",
    "\n",
    "# mordred\n",
    "Mord_19 = read_des(os.path.join(des_path, 'Mordred/19_data'))\n",
    "Mord_18 = read_des(os.path.join(des_path, 'Mordred/18_data'))\n",
    "Mord_sm = read_des(os.path.join(des_path, 'Mordred/suzuki'))\n",
    "\n",
    "# unimol\n",
    "UniMol_19 = read_des(os.path.join(des_path, 'unimol/19_data'))\n",
    "UniMol_18 = read_des(os.path.join(des_path, 'unimol/18_data'))\n",
    "UniMol_sm = read_des(os.path.join(des_path, 'unimol/suzuki'))\n",
    "\n",
    "# X_desc = rdkit_descriptors(df['full_smi'])\n",
    "# desc_dim = X_desc.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理 RDKit 特征数据\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def clean_rdkit_features(X):\n",
    "    imp = SimpleImputer(strategy='mean')\n",
    "    scaler = StandardScaler()\n",
    "    X_clean = imp.fit_transform(X)\n",
    "    X_scaled = scaler.fit_transform(X_clean)\n",
    "    return X_scaled\n",
    "\n",
    "def evaluate_rf(X, y, n_runs=30, n_estimators=500, max_depth=25, min_samples_split=2):\n",
    "    r2_scores, maes, times = [], [], []\n",
    "\n",
    "    for _ in tqdm(range(n_runs)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        duration = time.time() - start\n",
    "\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        maes.append(mean_absolute_error(y_test, y_pred))\n",
    "        times.append(duration)\n",
    "\n",
    "    return {\n",
    "        'R2_mean': np.mean(r2_scores),\n",
    "        'R2_std': np.std(r2_scores),\n",
    "        'MAE_mean': np.mean(maes),\n",
    "        'MAE_std': np.std(maes),\n",
    "        'Time_mean': np.mean(times),\n",
    "        'Time_std': np.std(times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [05:24<48:40, 108.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, (Xf, params) \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m evaluate_rf(Xf, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     23\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([\n\u001b[0;32m     24\u001b[0m         name, Xf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2_std\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     25\u001b[0m         result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE_std\u001b[39m\u001b[38;5;124m'\u001b[39m], result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime_std\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m     ])\n\u001b[0;32m     28\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature_Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDim\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR2_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR2_std\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE_std\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime_std\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m ])\n",
      "Cell \u001b[1;32mIn[31], line 27\u001b[0m, in \u001b[0;36mevaluate_rf\u001b[1;34m(X, y, n_runs, n_estimators, max_depth, min_samples_split)\u001b[0m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\n\u001b[0;32m     19\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[0;32m     20\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     29\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "#运行对比 只使用随机森林模型\n",
    "# load data\n",
    "X_onehot = OHE_18\n",
    "X_fp = FP_18\n",
    "X_desc = RDKit_18\n",
    "X_mord = Mord_18\n",
    "X_unimol = UniMol_18\n",
    "y = df_18['Output'].values\n",
    "\n",
    "features = {\n",
    "    # \"OneHot_SMILES\": (X_onehot, {\"n_estimators\": 300, \"max_depth\": 10, \"min_samples_split\": 2}),\n",
    "    # \"Fingerprint\": (X_fp, {\"n_estimators\": 300, \"max_depth\": 20, \"min_samples_split\": 4}),\n",
    "    # \"RDKit_Descriptors\": (X_desc, {\"n_estimators\": 300, \"max_depth\": 15, \"min_samples_split\": 3}),\n",
    "    # \"Mordred\": (X_mord, {\"n_estimators\": 300, \"max_depth\": 18, \"min_samples_split\": 3}),\n",
    "    \"Uni-Mol\": (X_unimol, {\"n_estimators\": 300, \"max_depth\": 15, \"min_samples_split\": 2}),\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, (Xf, params) in features.items():\n",
    "    result = evaluate_rf(Xf, y, **params)\n",
    "    results.append([\n",
    "        name, Xf.shape[1], result['R2_mean'], result['R2_std'],\n",
    "        result['MAE_mean'], result['MAE_std'], result['Time_mean'], result['Time_std']\n",
    "    ])\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[\n",
    "    \"Feature_Type\", \"Dim\",\n",
    "    \"R2_mean\", \"R2_std\", \"MAE_mean\", \"MAE_std\",\"Time_mean\", \"Time_std\"\n",
    "])\n",
    "print(result_df)\n",
    "# result_df.to_csv('results/ADD/18_science_RF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.46it/s]\n",
      "100%|██████████| 30/30 [02:18<00:00,  4.62s/it]\n",
      "100%|██████████| 30/30 [28:22<00:00, 56.74s/it]\n",
      "100%|██████████| 30/30 [52:59<00:00, 105.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature_Type    Dim   R2_mean    R2_std  MAE_mean   MAE_std   Time_mean  \\\n",
      "0  OneHot_SMILES     33  0.767226  0.013337  0.097088  0.002508    0.403916   \n",
      "1    Fingerprint  10240  0.843946  0.011125  0.076792  0.002454    4.471080   \n",
      "2        Mordred   5116  0.846380  0.008972  0.076557  0.001874   56.664036   \n",
      "3        Uni-Mol   2560  0.846607  0.009998  0.076785  0.002237  105.933555   \n",
      "\n",
      "   Time_std  \n",
      "0  0.004914  \n",
      "1  0.170679  \n",
      "2  0.578813  \n",
      "3  1.167414  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "#运行对比 只使用随机森林模型\n",
    "# load data\n",
    "X_onehot = OHE_sm\n",
    "X_fp = FP_sm\n",
    "X_desc = RDkit_sm\n",
    "X_mord = Mord_sm\n",
    "X_unimol = UniMol_sm\n",
    "y = df_sm['Output'].values\n",
    "\n",
    "features = {\n",
    "    \"OneHot_SMILES\": (X_onehot, {\"n_estimators\": 300, \"max_depth\": 10, \"min_samples_split\": 2}),\n",
    "    \"Fingerprint\": (X_fp, {\"n_estimators\": 300, \"max_depth\": 20, \"min_samples_split\": 4}),\n",
    "    # \"RDKit_Descriptors\": (X_desc, {\"n_estimators\": 300, \"max_depth\": 15, \"min_samples_split\": 3}),\n",
    "    \"Mordred\": (X_mord, {\"n_estimators\": 300, \"max_depth\": 18, \"min_samples_split\": 3}),\n",
    "    \"Uni-Mol\": (X_unimol, {\"n_estimators\": 500, \"max_depth\": 25, \"min_samples_split\": 2}),\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, (Xf, params) in features.items():\n",
    "    result = evaluate_rf(Xf, y, **params)\n",
    "    results.append([\n",
    "        name, Xf.shape[1], result['R2_mean'], result['R2_std'],\n",
    "        result['MAE_mean'], result['MAE_std'], result['Time_mean'], result['Time_std']\n",
    "    ])\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[\n",
    "    \"Feature_Type\", \"Dim\",\n",
    "    \"R2_mean\", \"R2_std\", \"MAE_mean\", \"MAE_std\",\"Time_mean\", \"Time_std\"\n",
    "])\n",
    "print(result_df)\n",
    "result_df.to_csv('results/ADD/18_science_RF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path and csv files\n",
    "DATA_DIR = './data/19_science/'\n",
    "# OUT_DIR = 'out/models_unimol_infer'+datetime.now().strftime('%y%m%d%H%M')+'/'\n",
    "# 分子特征\n",
    "INPUTS_Catlyst_repr = 'Catalyst_smirepr.csv'  # Unscaled  data \n",
    "INPUTS_Imine_repr = 'Imine_smirepr.csv'\n",
    "INPUTS_Thiol_repr = 'Thiol_smirepr.csv'\n",
    "INPUTS_Origin_DF = '19_science_total.csv'\n",
    "\n",
    "inputs_Catlyst_repr = pd.read_csv(DATA_DIR + INPUTS_Catlyst_repr)\n",
    "inputs_Imine_repr = pd.read_csv(DATA_DIR + INPUTS_Imine_repr)\n",
    "inputs_Thiol_repr = pd.read_csv(DATA_DIR + INPUTS_Thiol_repr)\n",
    "delta_G = pd.read_csv(DATA_DIR + INPUTS_Origin_DF)['Output']\n",
    "\n",
    "inputs = np.concatenate([inputs_Catlyst_repr,inputs_Imine_repr,inputs_Thiol_repr],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [19:59<00:00, 120.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_mean:0.9065780622226542 R2_std: 0.007746701650625964 MAE_mean: 0.14800891649328551 MAE_std: 0.006035723749478535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "new_split = 1\n",
    "\n",
    "predictions = []\n",
    "r2_values = []\n",
    "rmse_values = []\n",
    "\n",
    "OUT_DIR = 'out/19_science/'\n",
    "for _ in tqdm(range(10)):\n",
    "    if new_split == 1:\n",
    "        # 随机生成train和test的indices\n",
    "        total_cleaned_len = len(inputs_Catlyst_repr)\n",
    "        train_index = random.sample(range(total_cleaned_len), int(600))#论文中用了600个作为训练集 int(1075*0.7)\n",
    "        train_indices = np.zeros((total_cleaned_len)).astype(np.bool_)\n",
    "        for tcl in range(total_cleaned_len):\n",
    "            if tcl in train_index:\n",
    "                train_indices[tcl] = True\n",
    "        test_indices = ~ train_indices\n",
    "        np.savetxt(OUT_DIR + 'clean_data_train_indices.csv', train_indices, delimiter = ',')\n",
    "        np.savetxt(OUT_DIR + 'clean_data_test_indices.csv', test_indices, delimiter = ',')\n",
    "\n",
    "    elif new_split == 0:\n",
    "        # read saved train indices and test indices\n",
    "        train_indices = np.loadtxt(OUT_DIR + 'clean_data_train_indices.csv',dtype='bool', delimiter=',')\n",
    "        test_indices = np.loadtxt(OUT_DIR + 'clean_data_test_indices.csv',dtype='bool', delimiter=',')\n",
    "\n",
    "    # Load yield data\n",
    "    delta2G = np.array(delta_G)\n",
    "    delta2G = delta2G.flatten()\n",
    "    delta2G = np.nan_to_num(delta2G, nan=0)\n",
    "    # print('len(inputs): ', len(inputs))\n",
    "    # Use the indices to generate train/test sets\n",
    "    X_train = inputs[train_indices]\n",
    "    #X_train = inputs[:2]\n",
    "    y_train = delta2G[train_indices]\n",
    "    featuresTrain = torch.from_numpy(X_train)\n",
    "    targetsTrain = torch.from_numpy(y_train)\n",
    "    batch_size = len(X_train)\n",
    "    # print('batch_size: ', batch_size)\n",
    "\n",
    "    X_test = inputs[test_indices]\n",
    "    y_test = delta2G[test_indices]\n",
    "    featuresTest = torch.from_numpy(X_test)\n",
    "    targetsTest = torch.from_numpy(y_test)#.type(torch.LongTensor)\n",
    "    batch_size_test = len(X_test)\n",
    "    # print('batch_size_test: ', batch_size_test)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "    test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size_test, shuffle = False)\n",
    "\n",
    "    model =  RandomForestRegressor(n_estimators=300,random_state=42, max_depth=25, min_samples_split=2)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    # calculate an R-squared and RMSE values\n",
    "    r_squared = r2_score(y_test, preds)\n",
    "    r2_values.append(r_squared)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse_values.append(mae)\n",
    "# print(r_squared, mae)\n",
    "print(f'R2_mean:{np.mean(r2_values)}' ,f'R2_std: {np.std(r2_values)}',f'MAE_mean: {np.mean(rmse_values)}',f'MAE_std: {np.std(rmse_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18_science Buchwald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [4:06:43<00:00, 493.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_mean:0.9198322302837165 R2_std: 0.009513109441769594 MAE_mean: 5.06341686222863 MAE_std: 0.19155827265144476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_split = 1\n",
    "\n",
    "predictions = []\n",
    "r2_values = []\n",
    "rmse_values = []\n",
    "\n",
    "DATA_DIR = './data/18_science/'\n",
    "# OUT_DIR = 'out/models_unimol_infer'+datetime.now().strftime('%y%m%d%H%M')+'/'\n",
    "\n",
    "INPUTS_Aryl_halide_repr = 'Aryl haliderepr.csv'  # Unscaled  data \n",
    "INPUTS_Additive_repr = 'Additiverepr.csv'\n",
    "INPUTS_Base_repr = 'Baserepr.csv'\n",
    "INPUTS_Ligand_repr = 'Ligandrepr.csv'\n",
    "INPUTS_Origin_DF = '18 science_original_chem.xlsx'\n",
    "\n",
    "inputs_Aryl_halide_repr = pd.read_csv(DATA_DIR + INPUTS_Aryl_halide_repr)\n",
    "inputs_Additive_repr = pd.read_csv(DATA_DIR + INPUTS_Additive_repr)\n",
    "inputs_Base_repr = pd.read_csv(DATA_DIR + INPUTS_Base_repr)\n",
    "inputs_Ligand_repr = pd.read_csv(DATA_DIR + INPUTS_Ligand_repr)\n",
    "yields = pd.read_excel(DATA_DIR + INPUTS_Origin_DF)['Output']\n",
    "\n",
    "inputs = np.concatenate([inputs_Aryl_halide_repr ,inputs_Additive_repr,inputs_Base_repr,inputs_Ligand_repr],axis=1)\n",
    "\n",
    "OUT_DIR = 'out/18_science/'\n",
    "for _ in tqdm(range(30)):\n",
    "    if new_split == 1:\n",
    "        # 随机生成train和test的indices\n",
    "        total_cleaned_len = len(inputs_Aryl_halide_repr)\n",
    "        train_index = random.sample(range(total_cleaned_len), int(3955*0.7))\n",
    "        train_indices = np.zeros((total_cleaned_len)).astype(np.bool_)\n",
    "        for tcl in range(total_cleaned_len):\n",
    "            if tcl in train_index:\n",
    "                train_indices[tcl] = True\n",
    "        test_indices = ~ train_indices\n",
    "        np.savetxt(OUT_DIR + 'clean_data_train_indices.csv', train_indices, delimiter = ',')\n",
    "        np.savetxt(OUT_DIR + 'clean_data_test_indices.csv', test_indices, delimiter = ',')\n",
    "\n",
    "    elif new_split == 0:\n",
    "        # read saved train indices and test indices\n",
    "        train_indices = np.loadtxt(OUT_DIR + 'clean_data_train_indices.csv',dtype='bool', delimiter=',')\n",
    "        test_indices = np.loadtxt(OUT_DIR + 'clean_data_test_indices.csv',dtype='bool', delimiter=',')\n",
    "\n",
    "    # Load yield data\n",
    "    yields = np.array(yields)\n",
    "    yields = yields.flatten()\n",
    "    yields = np.nan_to_num(yields, nan=0)\n",
    "    # print('len(inputs): ', len(inputs))\n",
    "    # Use the indices to generate train/test sets\n",
    "    X_train = inputs[train_indices]\n",
    "    #X_train = inputs[:2]\n",
    "    y_train = yields[train_indices]\n",
    "    featuresTrain = torch.from_numpy(X_train)\n",
    "    targetsTrain = torch.from_numpy(y_train)\n",
    "    batch_size = len(X_train)\n",
    "    # print('batch_size: ', batch_size)\n",
    "\n",
    "    X_test = inputs[test_indices]\n",
    "    y_test = yields[test_indices]\n",
    "    featuresTest = torch.from_numpy(X_test)\n",
    "    targetsTest = torch.from_numpy(y_test)#.type(torch.LongTensor)\n",
    "    batch_size_test = len(X_test)\n",
    "    # print('batch_size_test: ', batch_size_test)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "    test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size_test, shuffle = False)\n",
    "\n",
    "    model =  RandomForestRegressor(n_estimators=300,random_state=42, max_depth=25, min_samples_split=2)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    # calculate an R-squared and RMSE values\n",
    "    r_squared = r2_score(y_test, preds)\n",
    "    r2_values.append(r_squared)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse_values.append(mae)\n",
    "# print(r_squared, mae)\n",
    "print(f'R2_mean:{np.mean(r2_values)}' ,f'R2_std: {np.std(r2_values)}',f'MAE_mean: {np.mean(rmse_values)}',f'MAE_std: {np.std(rmse_values)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## suzuki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [4:44:39<00:00, 569.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_mean:0.8476579457154416 R2_std: 0.008287351257785949 MAE_mean: 0.07645545984141394 MAE_std: 0.002079452394050178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_split = 1\n",
    "\n",
    "predictions = []\n",
    "r2_values = []\n",
    "rmse_values = []\n",
    "\n",
    "DATA_DIR = './data/suzuki/'\n",
    "\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "# 分子特征\n",
    "INPUTS_reactant1_repr = 'reactant_1repr.csv'  # Unscaled  data \n",
    "INPUTS_reactant2_repr = 'reactant_2repr.csv'\n",
    "INPUTS_reagent1_repr = 'reagent_1repr.csv'\n",
    "INPUTS_solvent1_repr = 'solvent_1repr.csv'\n",
    "INPUTS_ligand_repr = 'ligandrepr.csv'\n",
    "INPUTS_Origin_DF = 'suzuki.xlsx'\n",
    "\n",
    "inputs_reactant1_repr = pd.read_csv(DATA_DIR + INPUTS_reactant1_repr)\n",
    "inputs_reactant2_repr = pd.read_csv(DATA_DIR + INPUTS_reactant2_repr)\n",
    "inputs_reagent1_repr = pd.read_csv(DATA_DIR + INPUTS_reagent1_repr)\n",
    "inputs_solvent1_repr = pd.read_csv(DATA_DIR + INPUTS_solvent1_repr)\n",
    "inputs_Ligand_repr = pd.read_csv(DATA_DIR + INPUTS_ligand_repr)\n",
    "df_ori = pd.read_excel(DATA_DIR + INPUTS_Origin_DF)\n",
    "df = df_ori.dropna(axis=0, how='any')\n",
    "inputs = np.concatenate([inputs_reactant1_repr ,inputs_reactant2_repr,inputs_reagent1_repr,inputs_solvent1_repr,inputs_Ligand_repr],axis=1)\n",
    "yields = df['Output']\n",
    "\n",
    "\n",
    "OUT_DIR = 'out/suzuki/'\n",
    "for _ in tqdm(range(30)):\n",
    "    if new_split == 1:\n",
    "        # 随机生成train和test的indices\n",
    "        total_cleaned_len = len(inputs_reactant1_repr)\n",
    "        train_index = random.sample(range(total_cleaned_len), int(total_cleaned_len*0.7))\n",
    "        train_indices = np.zeros((total_cleaned_len)).astype(np.bool_)\n",
    "        for tcl in range(total_cleaned_len):\n",
    "            if tcl in train_index:\n",
    "                train_indices[tcl] = True\n",
    "        test_indices = ~ train_indices\n",
    "        np.savetxt(OUT_DIR + 'clean_data_train_indices.csv', train_indices, delimiter = ',')\n",
    "        np.savetxt(OUT_DIR + 'clean_data_test_indices.csv', test_indices, delimiter = ',')\n",
    "\n",
    "    elif new_split == 0:\n",
    "        # read saved train indices and test indices\n",
    "        train_indices = np.loadtxt(OUT_DIR + 'clean_data_train_indices.csv',dtype='bool', delimiter=',')\n",
    "        test_indices = np.loadtxt(OUT_DIR + 'clean_data_test_indices.csv',dtype='bool', delimiter=',')\n",
    "\n",
    "    # Load yield data\n",
    "    yields = np.array(yields)\n",
    "    yields = yields.flatten()\n",
    "    yields = np.nan_to_num(yields, nan=0)\n",
    "    # print('len(inputs): ', len(inputs))\n",
    "    # Use the indices to generate train/test sets\n",
    "    X_train = inputs[train_indices]\n",
    "    #X_train = inputs[:2]\n",
    "    y_train = yields[train_indices]\n",
    "    featuresTrain = torch.from_numpy(X_train)\n",
    "    targetsTrain = torch.from_numpy(y_train)\n",
    "    batch_size = len(X_train)\n",
    "    # print('batch_size: ', batch_size)\n",
    "\n",
    "    X_test = inputs[test_indices]\n",
    "    y_test = yields[test_indices]\n",
    "    featuresTest = torch.from_numpy(X_test)\n",
    "    targetsTest = torch.from_numpy(y_test)#.type(torch.LongTensor)\n",
    "    batch_size_test = len(X_test)\n",
    "    # print('batch_size_test: ', batch_size_test)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "    test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size_test, shuffle = False)\n",
    "\n",
    "    model =  RandomForestRegressor(n_estimators=300,random_state=42, max_depth=25, min_samples_split=2)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    # calculate an R-squared and RMSE values\n",
    "    r_squared = r2_score(y_test, preds)\n",
    "    r2_values.append(r_squared)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse_values.append(mae)\n",
    "# print(r_squared, mae)\n",
    "print(f'R2_mean:{np.mean(r2_values)}' ,f'R2_std: {np.std(r2_values)}',f'MAE_mean: {np.mean(rmse_values)}',f'MAE_std: {np.std(rmse_values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unimol_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
