{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70/30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "\n",
    "# Import relevant scikit-learn modules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import ModelFits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Reprs/Solvent_Repr.csv',\n",
       " '../Reprs/Solvent_Repr_160.csv',\n",
       " '../Reprs/Solvent_Repr_180.csv',\n",
       " '../Reprs/sol_oh.csv',\n",
       " '../Reprs/sol_oh_180.csv',\n",
       " '../Reprs/UniMolRepr.csv',\n",
       " '../Reprs/UniMolRepr_160.csv',\n",
       " '../Reprs/UniMolRepr_180.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read .csv format data\n",
    "Data_path = '../Data/data_160_ori.csv'\n",
    "REPR_DIR = '../Reprs'\n",
    "\n",
    "repr_path= []\n",
    "for file in os.listdir(REPR_DIR):\n",
    "    if '.csv' in file:\n",
    "        path = REPR_DIR+ '/' + file\n",
    "        repr_path.append(path)\n",
    "\n",
    "repr_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Data_path, index_col=0)\n",
    "unimol_repr = pd.read_csv(repr_path[-2],index_col=0)\n",
    "sol_repr = pd.read_csv(repr_path[1],index_col=0)\n",
    "sol_oh = pd.read_csv(repr_path[3],index_col=0)\n",
    "# sol_oh = sol_oh[0:160]\n",
    "\n",
    "# concat required features\n",
    "fea_df = pd.concat([unimol_repr,sol_repr], axis=1)\n",
    "\n",
    "# Extract label colunm\n",
    "label_df = df['yields_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test yields\n",
    "# label Encoding\n",
    "le = LabelEncoder()\n",
    "label_le = le.fit_transform(label_df) #  0: high 1: low, 2: medium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_26880\\3273759680.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'medium' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  ee.iloc[i] = 'medium'\n"
     ]
    }
   ],
   "source": [
    "ee = df['ee'].copy()\n",
    "\n",
    "low = []\n",
    "med = []\n",
    "high = []\n",
    "\n",
    "for i in range(len(ee)):\n",
    "    if ee[i] >= 80:\n",
    "        high.append(i)\n",
    "        ee.iloc[i] = 'high'\n",
    "    elif ee[i] <= 20:\n",
    "        low.append(i)\n",
    "        ee.iloc[i] = 'low'\n",
    "    else:\n",
    "        med.append(i)\n",
    "        ee.iloc[i] = 'medium'\n",
    "label_df2 = ee\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "label_le2 = le2.fit_transform(label_df2) # medium 2, low 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fea_df\n",
    "y = label_le\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 3  1  0]\n",
      " [ 0 37  1]\n",
      " [ 0  2  4]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.93      0.97      0.95        38\n",
      "           2       0.80      0.67      0.73         6\n",
      "\n",
      "    accuracy                           0.92        48\n",
      "   macro avg       0.91      0.80      0.84        48\n",
      "weighted avg       0.92      0.92      0.91        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier on the resampled training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the original test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Random State: 40**********************\n",
      "Confusion Matrix:\n",
      " [[ 4  2  1]\n",
      " [ 0 29  0]\n",
      " [ 1  7  4]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.67         7\n",
      "           1       0.76      1.00      0.87        29\n",
      "           2       0.80      0.33      0.47        12\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.79      0.63      0.67        48\n",
      "weighted avg       0.78      0.77      0.74        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 41**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  2  1]\n",
      " [ 0 30  3]\n",
      " [ 2  5  2]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       0.81      0.91      0.86        33\n",
      "           2       0.33      0.22      0.27         9\n",
      "\n",
      "    accuracy                           0.73        48\n",
      "   macro avg       0.58      0.54      0.56        48\n",
      "weighted avg       0.69      0.73      0.71        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 42**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  1  0]\n",
      " [ 0 37  1]\n",
      " [ 1  1  4]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75         4\n",
      "           1       0.95      0.97      0.96        38\n",
      "           2       0.80      0.67      0.73         6\n",
      "\n",
      "    accuracy                           0.92        48\n",
      "   macro avg       0.83      0.80      0.81        48\n",
      "weighted avg       0.91      0.92      0.91        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 43**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0]\n",
      " [ 0 36  2]\n",
      " [ 0  6  1]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       0.86      0.95      0.90        38\n",
      "           2       0.33      0.14      0.20         7\n",
      "\n",
      "    accuracy                           0.83        48\n",
      "   macro avg       0.73      0.70      0.70        48\n",
      "weighted avg       0.79      0.83      0.80        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 44**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  3  2]\n",
      " [ 1 26  3]\n",
      " [ 0  5  5]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.38      0.50         8\n",
      "           1       0.76      0.87      0.81        30\n",
      "           2       0.50      0.50      0.50        10\n",
      "\n",
      "    accuracy                           0.71        48\n",
      "   macro avg       0.67      0.58      0.60        48\n",
      "weighted avg       0.71      0.71      0.70        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 45**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  0  1]\n",
      " [ 1 33  1]\n",
      " [ 1  2  6]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67         4\n",
      "           1       0.94      0.94      0.94        35\n",
      "           2       0.75      0.67      0.71         9\n",
      "\n",
      "    accuracy                           0.88        48\n",
      "   macro avg       0.76      0.79      0.77        48\n",
      "weighted avg       0.88      0.88      0.88        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 46**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  3  1]\n",
      " [ 0 28  3]\n",
      " [ 0  5  5]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.43      0.60         7\n",
      "           1       0.78      0.90      0.84        31\n",
      "           2       0.56      0.50      0.53        10\n",
      "\n",
      "    accuracy                           0.75        48\n",
      "   macro avg       0.78      0.61      0.65        48\n",
      "weighted avg       0.76      0.75      0.74        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 47**********************\n",
      "Confusion Matrix:\n",
      " [[ 5  0  1]\n",
      " [ 0 27  5]\n",
      " [ 2  2  6]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       0.93      0.84      0.89        32\n",
      "           2       0.50      0.60      0.55        10\n",
      "\n",
      "    accuracy                           0.79        48\n",
      "   macro avg       0.72      0.76      0.73        48\n",
      "weighted avg       0.81      0.79      0.80        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 48**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  1  1]\n",
      " [ 1 32  4]\n",
      " [ 0  5  1]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.84      0.86      0.85        37\n",
      "           2       0.17      0.17      0.17         6\n",
      "\n",
      "    accuracy                           0.75        48\n",
      "   macro avg       0.59      0.54      0.56        48\n",
      "weighted avg       0.75      0.75      0.75        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 49**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0]\n",
      " [ 1 34  1]\n",
      " [ 0  6  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       0.85      0.94      0.89        36\n",
      "           2       0.75      0.33      0.46         9\n",
      "\n",
      "    accuracy                           0.83        48\n",
      "   macro avg       0.78      0.76      0.74        48\n",
      "weighted avg       0.82      0.83      0.81        48\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(40, 50):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "    class_weights = {0: 3, 1: 1, 2: 2} # O:High More important; 1: Medium Less important; 2: Low last important\n",
    "    rf_classifier = RandomForestClassifier(random_state=i, class_weight=class_weights) # class_weight=class_weights\n",
    "\n",
    "    # Train the classifier on the sampled training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on the original test set\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f'*********************Random State: {i}**********************')\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Random State: 40**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  0  2]\n",
      " [ 1  9  4]\n",
      " [ 0  2 27]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67         5\n",
      "           1       0.82      0.64      0.72        14\n",
      "           2       0.82      0.93      0.87        29\n",
      "\n",
      "    accuracy                           0.81        48\n",
      "   macro avg       0.80      0.72      0.75        48\n",
      "weighted avg       0.81      0.81      0.81        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 41**********************\n",
      "Confusion Matrix:\n",
      " [[ 0  0  0]\n",
      " [ 1  8  4]\n",
      " [ 5  2 28]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.80      0.62      0.70        13\n",
      "           2       0.88      0.80      0.84        35\n",
      "\n",
      "    accuracy                           0.75        48\n",
      "   macro avg       0.56      0.47      0.51        48\n",
      "weighted avg       0.85      0.75      0.80        48\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************Random State: 42**********************\n",
      "Confusion Matrix:\n",
      " [[ 2  0  2]\n",
      " [ 0  8  8]\n",
      " [ 0  3 25]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.73      0.50      0.59        16\n",
      "           2       0.71      0.89      0.79        28\n",
      "\n",
      "    accuracy                           0.73        48\n",
      "   macro avg       0.81      0.63      0.68        48\n",
      "weighted avg       0.74      0.73      0.72        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 43**********************\n",
      "Confusion Matrix:\n",
      " [[ 1  1  1]\n",
      " [ 0  9  5]\n",
      " [ 2  9 20]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.33      0.33         3\n",
      "           1       0.47      0.64      0.55        14\n",
      "           2       0.77      0.65      0.70        31\n",
      "\n",
      "    accuracy                           0.62        48\n",
      "   macro avg       0.53      0.54      0.53        48\n",
      "weighted avg       0.66      0.62      0.63        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 44**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  1  1]\n",
      " [ 0  5  9]\n",
      " [ 2  3 24]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60         5\n",
      "           1       0.56      0.36      0.43        14\n",
      "           2       0.71      0.83      0.76        29\n",
      "\n",
      "    accuracy                           0.67        48\n",
      "   macro avg       0.62      0.59      0.60        48\n",
      "weighted avg       0.65      0.67      0.65        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 45**********************\n",
      "Confusion Matrix:\n",
      " [[ 0  0  3]\n",
      " [ 0 11  6]\n",
      " [ 2  4 22]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.73      0.65      0.69        17\n",
      "           2       0.71      0.79      0.75        28\n",
      "\n",
      "    accuracy                           0.69        48\n",
      "   macro avg       0.48      0.48      0.48        48\n",
      "weighted avg       0.67      0.69      0.68        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 46**********************\n",
      "Confusion Matrix:\n",
      " [[ 1  0  3]\n",
      " [ 0  7  5]\n",
      " [ 0  7 25]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40         4\n",
      "           1       0.50      0.58      0.54        12\n",
      "           2       0.76      0.78      0.77        32\n",
      "\n",
      "    accuracy                           0.69        48\n",
      "   macro avg       0.75      0.54      0.57        48\n",
      "weighted avg       0.71      0.69      0.68        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 47**********************\n",
      "Confusion Matrix:\n",
      " [[ 4  0  2]\n",
      " [ 0  7  9]\n",
      " [ 1  1 24]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.88      0.44      0.58        16\n",
      "           2       0.69      0.92      0.79        26\n",
      "\n",
      "    accuracy                           0.73        48\n",
      "   macro avg       0.79      0.68      0.70        48\n",
      "weighted avg       0.76      0.73      0.71        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 48**********************\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0]\n",
      " [ 0 10  7]\n",
      " [ 2  3 23]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75         3\n",
      "           1       0.77      0.59      0.67        17\n",
      "           2       0.77      0.82      0.79        28\n",
      "\n",
      "    accuracy                           0.75        48\n",
      "   macro avg       0.71      0.80      0.74        48\n",
      "weighted avg       0.76      0.75      0.75        48\n",
      "\n",
      "-------------------------------------------------------------\n",
      "*********************Random State: 49**********************\n",
      "Confusion Matrix:\n",
      " [[ 0  0  1]\n",
      " [ 0  7  5]\n",
      " [ 0  6 29]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.54      0.58      0.56        12\n",
      "           2       0.83      0.83      0.83        35\n",
      "\n",
      "    accuracy                           0.75        48\n",
      "   macro avg       0.46      0.47      0.46        48\n",
      "weighted avg       0.74      0.75      0.74        48\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(40, 50):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, label_le2, test_size=0.3, random_state=i)\n",
    "    class_weights = {0: 3, 1: 1, 2: 2} # O:High More important; 1: Medium Less important; 2: Low last important\n",
    "    rf_classifier = RandomForestClassifier(random_state=i, class_weight=class_weights) # class_weight=class_weights\n",
    "\n",
    "    # Train the classifier on the sampled training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on the original test set\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f'*********************Random State: {i}**********************')\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分层抽样 交叉验证 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每次划分的训练集评分:\n",
      "第1次训练集准确率: 0.9514\n",
      "第1次训练集精确度: 0.9178\n",
      "第1次训练集召回率: 0.9683\n",
      "第1次训练集F1分数: 0.9410\n",
      "第2次训练集准确率: 0.9028\n",
      "第2次训练集精确度: 0.8470\n",
      "第2次训练集召回率: 0.8930\n",
      "第2次训练集F1分数: 0.8676\n",
      "第3次训练集准确率: 0.9444\n",
      "第3次训练集精确度: 0.9035\n",
      "第3次训练集召回率: 0.9390\n",
      "第3次训练集F1分数: 0.9189\n",
      "第4次训练集准确率: 0.9306\n",
      "第4次训练集精确度: 0.8837\n",
      "第4次训练集召回率: 0.9422\n",
      "第4次训练集F1分数: 0.9099\n",
      "第5次训练集准确率: 0.9514\n",
      "第5次训练集精确度: 0.9344\n",
      "第5次训练集召回率: 0.9359\n",
      "第5次训练集F1分数: 0.9322\n",
      "第6次训练集准确率: 0.9444\n",
      "第6次训练集精确度: 0.9161\n",
      "第6次训练集召回率: 0.9489\n",
      "第6次训练集F1分数: 0.9312\n",
      "第7次训练集准确率: 0.9306\n",
      "第7次训练集精确度: 0.8770\n",
      "第7次训练集召回率: 0.9315\n",
      "第7次训练集F1分数: 0.9016\n",
      "第8次训练集准确率: 0.9583\n",
      "第8次训练集精确度: 0.9191\n",
      "第8次训练集召回率: 0.9718\n",
      "第8次训练集F1分数: 0.9433\n",
      "第9次训练集准确率: 0.9167\n",
      "第9次训练集精确度: 0.8602\n",
      "第9次训练集召回率: 0.9355\n",
      "第9次训练集F1分数: 0.8926\n",
      "第10次训练集准确率: 0.9306\n",
      "第10次训练集精确度: 0.8808\n",
      "第10次训练集召回率: 0.9259\n",
      "第10次训练集F1分数: 0.8965\n",
      "\n",
      "每次划分的测试集评分:\n",
      "第1次测试集准确率: 0.7500\n",
      "第1次测试集精确度: 0.4167\n",
      "第1次测试集召回率: 0.4167\n",
      "第1次测试集F1分数: 0.4167\n",
      "第2次测试集准确率: 0.8125\n",
      "第2次测试集精确度: 0.5278\n",
      "第2次测试集召回率: 0.5000\n",
      "第2次测试集F1分数: 0.5093\n",
      "第3次测试集准确率: 0.6875\n",
      "第3次测试集精确度: 0.7333\n",
      "第3次测试集召回率: 0.7424\n",
      "第3次测试集F1分数: 0.7354\n",
      "第4次测试集准确率: 0.8750\n",
      "第4次测试集精确度: 0.8586\n",
      "第4次测试集召回率: 0.8586\n",
      "第4次测试集F1分数: 0.8586\n",
      "第5次测试集准确率: 0.6875\n",
      "第5次测试集精确度: 0.7333\n",
      "第5次测试集召回率: 0.6313\n",
      "第5次测试集F1分数: 0.6429\n",
      "第6次测试集准确率: 0.7500\n",
      "第6次测试集精确度: 0.4444\n",
      "第6次测试集召回率: 0.6364\n",
      "第6次测试集F1分数: 0.5121\n",
      "第7次测试集准确率: 0.7500\n",
      "第7次测试集精确度: 0.7727\n",
      "第7次测试集召回率: 0.7172\n",
      "第7次测试集F1分数: 0.6616\n",
      "第8次测试集准确率: 0.7500\n",
      "第8次测试集精确度: 0.6333\n",
      "第8次测试集召回率: 0.7172\n",
      "第8次测试集F1分数: 0.6635\n",
      "第9次测试集准确率: 0.7500\n",
      "第9次测试集精确度: 0.6333\n",
      "第9次测试集召回率: 0.7172\n",
      "第9次测试集F1分数: 0.6635\n",
      "第10次测试集准确率: 0.8125\n",
      "第10次测试集精确度: 0.7222\n",
      "第10次测试集召回率: 0.8283\n",
      "第10次测试集F1分数: 0.7444\n",
      "\n",
      "平均评分:\n",
      "平均训练集准确率: 0.9361\n",
      "平均训练集精确度: 0.8940\n",
      "平均训练集召回率: 0.9392\n",
      "平均训练集F1分数: 0.9135\n",
      "平均测试集准确率: 0.7625\n",
      "平均测试集精确度: 0.6476\n",
      "平均测试集召回率: 0.6765\n",
      "平均测试集F1分数: 0.6408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# model = RandomForestClassifier(\n",
    "#     n_estimators=100,         # 树的数量\n",
    "#     max_depth=10,             # 树的最大深度，防止过拟合\n",
    "#     min_samples_split=5,      # 内部节点再划分所需的最小样本数\n",
    "#     min_samples_leaf=4,       # 叶节点所需的最小样本数\n",
    "#     class_weight='balanced',  # 处理类别不均衡\n",
    "#     random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=93,         # 树的数量\n",
    "    max_depth=19,             # 树的最大深度，防止过拟合\n",
    "    min_samples_split=4,      # 内部节点再划分所需的最小样本数\n",
    "    min_samples_leaf=4,       # 叶节点所需的最小样本数\n",
    "    class_weight= {0: 4, 1: 1, 2: 2},  # 处理类别不均衡\n",
    "    max_features='sqrt',\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# 使用多个评分标准\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "\n",
    "X = fea_df\n",
    "y = label_le2\n",
    "\n",
    "# 执行交叉验证，获取各类评分\n",
    "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# 输出每次划分的训练集和测试集评分\n",
    "print(\"每次划分的训练集评分:\")\n",
    "for i in range(10):\n",
    "    print(f\"第{i+1}次训练集准确率: {cv_results['train_accuracy'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次训练集精确度: {cv_results['train_precision_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次训练集召回率: {cv_results['train_recall_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次训练集F1分数: {cv_results['train_f1_macro'][i]:.4f}\")\n",
    "\n",
    "print(\"\\n每次划分的测试集评分:\")\n",
    "for i in range(10):\n",
    "    print(f\"第{i+1}次测试集准确率: {cv_results['test_accuracy'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次测试集精确度: {cv_results['test_precision_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次测试集召回率: {cv_results['test_recall_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次测试集F1分数: {cv_results['test_f1_macro'][i]:.4f}\")\n",
    "\n",
    "# 输出平均评分\n",
    "print(\"\\n平均评分:\")\n",
    "print(f\"平均训练集准确率: {cv_results['train_accuracy'].mean():.4f}\")\n",
    "print(f\"平均训练集精确度: {cv_results['train_precision_macro'].mean():.4f}\")\n",
    "print(f\"平均训练集召回率: {cv_results['train_recall_macro'].mean():.4f}\")\n",
    "print(f\"平均训练集F1分数: {cv_results['train_f1_macro'].mean():.4f}\")\n",
    "\n",
    "print(f\"平均测试集准确率: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"平均测试集精确度: {cv_results['test_precision_macro'].mean():.4f}\")\n",
    "print(f\"平均测试集召回率: {cv_results['test_recall_macro'].mean():.4f}\")\n",
    "print(f\"平均测试集F1分数: {cv_results['test_f1_macro'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-17 19:48:27,022] A new study created in memory with name: no-name-ea830269-8e7d-4beb-8518-cede42cc3624\n",
      "[I 2025-01-17 19:48:27,167] Trial 0 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 155, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.7291666666666666.\n",
      "[I 2025-01-17 19:48:29,172] Trial 1 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 106, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 0 with value: 0.7291666666666666.\n",
      "[I 2025-01-17 19:48:29,246] Trial 2 finished with value: 0.75 and parameters: {'n_estimators': 77, 'max_depth': 15, 'min_samples_split': 4, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:29,368] Trial 3 finished with value: 0.75 and parameters: {'n_estimators': 88, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:29,508] Trial 4 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 143, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:31,457] Trial 5 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 133, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:32,929] Trial 6 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 124, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:35,120] Trial 7 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 175, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 8, 'max_features': None}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:36,171] Trial 8 finished with value: 0.75 and parameters: {'n_estimators': 69, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:36,286] Trial 9 finished with value: 0.75 and parameters: {'n_estimators': 131, 'max_depth': 19, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:36,373] Trial 10 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 59, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.75.\n",
      "[I 2025-01-17 19:48:36,519] Trial 11 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 93, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:36,667] Trial 12 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 90, 'max_depth': 17, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:36,831] Trial 13 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 99, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:37,006] Trial 14 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 109, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:37,097] Trial 15 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 51, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:37,253] Trial 16 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 87, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:37,419] Trial 17 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 116, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:37,718] Trial 18 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 195, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:37,845] Trial 19 finished with value: 0.75 and parameters: {'n_estimators': 92, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:37,977] Trial 20 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 75, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:38,143] Trial 21 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 102, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:38,291] Trial 22 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 94, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:38,420] Trial 23 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 68, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:38,595] Trial 24 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 115, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:38,758] Trial 25 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 98, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:38,881] Trial 26 finished with value: 0.75 and parameters: {'n_estimators': 81, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:39,126] Trial 27 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 145, 'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:39,269] Trial 28 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 119, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:39,375] Trial 29 finished with value: 0.75 and parameters: {'n_estimators': 65, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:39,508] Trial 30 finished with value: 0.75 and parameters: {'n_estimators': 106, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:39,687] Trial 31 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 110, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:39,850] Trial 32 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 99, 'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:40,003] Trial 33 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 84, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:40,162] Trial 34 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 106, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:40,419] Trial 35 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 162, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:41,825] Trial 36 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 78, 'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:41,960] Trial 37 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 129, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:42,197] Trial 38 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 139, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:43,682] Trial 39 finished with value: 0.75 and parameters: {'n_estimators': 91, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:43,863] Trial 40 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 111, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:43,965] Trial 41 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 59, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:44,062] Trial 42 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 54, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:44,185] Trial 43 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 74, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:44,387] Trial 44 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 124, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:45,823] Trial 45 finished with value: 0.75 and parameters: {'n_estimators': 89, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:45,895] Trial 46 finished with value: 0.75 and parameters: {'n_estimators': 54, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:46,050] Trial 47 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 98, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:46,255] Trial 48 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 120, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:46,376] Trial 49 finished with value: 0.75 and parameters: {'n_estimators': 83, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:47,796] Trial 50 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 72, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:47,944] Trial 51 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 84, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:48,127] Trial 52 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 105, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:48,243] Trial 53 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 64, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:48,406] Trial 54 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 94, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:48,567] Trial 55 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 89, 'max_depth': 19, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:48,656] Trial 56 finished with value: 0.75 and parameters: {'n_estimators': 50, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:48,807] Trial 57 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 110, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:48,913] Trial 58 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:49,117] Trial 59 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 115, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:49,247] Trial 60 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 79, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:49,416] Trial 61 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 115, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:49,561] Trial 62 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 96, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:49,740] Trial 63 finished with value: 0.75 and parameters: {'n_estimators': 128, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:49,925] Trial 64 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 121, 'max_depth': 14, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:50,146] Trial 65 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 137, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:50,319] Trial 66 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 104, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:50,550] Trial 67 finished with value: 0.75 and parameters: {'n_estimators': 152, 'max_depth': 16, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:50,709] Trial 68 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 88, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:52,664] Trial 69 finished with value: 0.75 and parameters: {'n_estimators': 110, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:52,774] Trial 70 finished with value: 0.75 and parameters: {'n_estimators': 93, 'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:53,047] Trial 71 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 176, 'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:53,344] Trial 72 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 192, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:53,668] Trial 73 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 197, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:53,829] Trial 74 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 102, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:54,014] Trial 75 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 113, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:54,264] Trial 76 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 169, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:54,411] Trial 77 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 86, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:54,622] Trial 78 finished with value: 0.75 and parameters: {'n_estimators': 148, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:54,810] Trial 79 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 107, 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:56,883] Trial 80 finished with value: 0.75 and parameters: {'n_estimators': 118, 'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,021] Trial 81 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 73, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,142] Trial 82 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 64, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,286] Trial 83 finished with value: 0.75 and parameters: {'n_estimators': 79, 'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,429] Trial 84 finished with value: 0.75 and parameters: {'n_estimators': 97, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,552] Trial 85 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 68, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,693] Trial 86 finished with value: 0.75 and parameters: {'n_estimators': 76, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,807] Trial 87 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 92, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:57,926] Trial 88 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 59, 'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:58,093] Trial 89 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 102, 'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:58,250] Trial 90 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 85, 'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:58,438] Trial 91 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 107, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:58,603] Trial 92 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 98, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:58,765] Trial 93 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 102, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:58,912] Trial 94 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 90, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:59,052] Trial 95 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 80, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:48:59,225] Trial 96 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 95, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:49:02,510] Trial 97 finished with value: 0.7291666666666666 and parameters: {'n_estimators': 186, 'max_depth': 19, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:49:02,675] Trial 98 finished with value: 0.75 and parameters: {'n_estimators': 126, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.7708333333333334.\n",
      "[I 2025-01-17 19:49:02,827] Trial 99 finished with value: 0.7708333333333334 and parameters: {'n_estimators': 132, 'max_depth': 17, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 11 with value: 0.7708333333333334.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 93, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      "Best accuracy: 0.7708\n"
     ]
    }
   ],
   "source": [
    "#超参数优化\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label_le2, test_size=0.3, random_state=i)\n",
    "\n",
    "def objective(trial):\n",
    "    # 超参数空间定义\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)  # 树的数量\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)  # 树的最大深度\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)  # 内部节点再划分所需的最小样本数\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)  # 叶节点所需的最小样本数\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]) # 特征选择方式\n",
    "\n",
    "    # 创建随机森林分类器\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 预测并计算准确率\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy  # 返回优化目标（这里是准确率）\n",
    "\n",
    "# 创建 Optuna 学习任务\n",
    "study = optuna.create_study(direction='maximize')  # 最大化目标函数（准确率）\n",
    "study.optimize(objective, n_trials=100)  # 进行 100 次超参数优化\n",
    "\n",
    "# 输出最优的超参数和对应的结果\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "print(f\"Best accuracy: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-18 10:50:49,305] A new study created in memory with name: no-name-39f1db3a-e339-4cd5-9b6e-9dbfbbb07543\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:50:52,824] Trial 0 finished with value: 0.79375 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'logistic', 'solver': 'lbfgs', 'alpha': 1.5348185645180055e-05, 'learning_rate': 'constant', 'max_iter': 329}. Best is trial 0 with value: 0.79375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (236) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:51:03,670] Trial 1 finished with value: 0.78125 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.013824278567203132, 'learning_rate': 'adaptive', 'max_iter': 236}. Best is trial 0 with value: 0.79375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:51:06,896] Trial 2 finished with value: 0.7875 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0008945763634229832, 'learning_rate': 'adaptive', 'max_iter': 358}. Best is trial 0 with value: 0.79375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (372) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:51:23,575] Trial 3 finished with value: 0.69375 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.09781538287326183, 'learning_rate': 'constant', 'max_iter': 372}. Best is trial 0 with value: 0.79375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:51:27,983] Trial 4 finished with value: 0.8 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0005816327582545659, 'learning_rate': 'adaptive', 'max_iter': 409}. Best is trial 4 with value: 0.8.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (291) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:51:34,156] Trial 5 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0005607775917705488, 'learning_rate': 'invscaling', 'max_iter': 291}. Best is trial 5 with value: 0.8375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:51:37,618] Trial 6 finished with value: 0.83125 and parameters: {'hidden_layer_sizes': (100,), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 0.03684866488165938, 'learning_rate': 'constant', 'max_iter': 468}. Best is trial 5 with value: 0.8375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:51:38,819] Trial 7 finished with value: 0.7625 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 0.00024514859407986217, 'learning_rate': 'constant', 'max_iter': 268}. Best is trial 5 with value: 0.8375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:51:40,118] Trial 8 finished with value: 0.80625 and parameters: {'hidden_layer_sizes': (100,), 'activation': 'logistic', 'solver': 'lbfgs', 'alpha': 0.00015158954880366542, 'learning_rate': 'adaptive', 'max_iter': 310}. Best is trial 5 with value: 0.8375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:51:44,595] Trial 9 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.014822359495937095, 'learning_rate': 'invscaling', 'max_iter': 211}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:51:49,071] Trial 10 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.006056162246760525, 'learning_rate': 'invscaling', 'max_iter': 211}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (276) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:51:54,861] Trial 11 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.004133158509816178, 'learning_rate': 'invscaling', 'max_iter': 276}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (208) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:51:59,264] Trial 12 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 6.679807163709665e-05, 'learning_rate': 'invscaling', 'max_iter': 208}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (278) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:05,077] Trial 13 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.002002253451983071, 'learning_rate': 'invscaling', 'max_iter': 278}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (247) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:10,259] Trial 14 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.017238142717217294, 'learning_rate': 'invscaling', 'max_iter': 247}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:52:11,836] Trial 15 finished with value: 0.79375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.022585026172409765, 'learning_rate': 'invscaling', 'max_iter': 239}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (245) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:15,849] Trial 16 finished with value: 0.64375 and parameters: {'hidden_layer_sizes': (100,), 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.05226210144885829, 'learning_rate': 'invscaling', 'max_iter': 245}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (203) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:20,158] Trial 17 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.010227622664974616, 'learning_rate': 'invscaling', 'max_iter': 203}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:52:22,437] Trial 18 finished with value: 0.8 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0027030016540291694, 'learning_rate': 'invscaling', 'max_iter': 497}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (391) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:29,287] Trial 19 finished with value: 0.46875 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0879127465269656, 'learning_rate': 'invscaling', 'max_iter': 391}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:52:34,879] Trial 20 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100,), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.009562468502356647, 'learning_rate': 'invscaling', 'max_iter': 327}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:41,111] Trial 21 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.00045917309676478315, 'learning_rate': 'invscaling', 'max_iter': 300}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:46,505] Trial 22 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.002137637701453645, 'learning_rate': 'invscaling', 'max_iter': 256}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:51,865] Trial 23 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0015575612431354296, 'learning_rate': 'invscaling', 'max_iter': 256}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (228) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:52:56,675] Trial 24 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.027991140342871142, 'learning_rate': 'invscaling', 'max_iter': 228}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (257) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:02,137] Trial 25 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.004828904331442355, 'learning_rate': 'invscaling', 'max_iter': 257}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:06,837] Trial 26 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.019537437730085393, 'learning_rate': 'invscaling', 'max_iter': 223}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:11,051] Trial 27 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.006469900453957882, 'learning_rate': 'invscaling', 'max_iter': 200}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:53:16,359] Trial 28 finished with value: 0.81875 and parameters: {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'lbfgs', 'alpha': 0.050090483581930904, 'learning_rate': 'constant', 'max_iter': 322}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (434) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:24,121] Trial 29 finished with value: 0.83125 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.002997314797143311, 'learning_rate': 'adaptive', 'max_iter': 434}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:53:27,503] Trial 30 finished with value: 0.79375 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'logistic', 'solver': 'lbfgs', 'alpha': 1.2282736550702533e-05, 'learning_rate': 'constant', 'max_iter': 351}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (256) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:32,821] Trial 31 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0012393180245907755, 'learning_rate': 'invscaling', 'max_iter': 256}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (248) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:38,038] Trial 32 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.013311251433410519, 'learning_rate': 'invscaling', 'max_iter': 248}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (222) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:42,745] Trial 33 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 3.086272102759781e-05, 'learning_rate': 'invscaling', 'max_iter': 222}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:53:55,216] Trial 34 finished with value: 0.83125 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0013279238592840137, 'learning_rate': 'adaptive', 'max_iter': 289}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (263) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:53:59,879] Trial 35 finished with value: 0.4375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0020658258720572804, 'learning_rate': 'invscaling', 'max_iter': 263}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:54:12,446] Trial 36 finished with value: 0.83125 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0002400495645641766, 'learning_rate': 'invscaling', 'max_iter': 229}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (307) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:54:18,938] Trial 37 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.0074443046001806314, 'learning_rate': 'adaptive', 'max_iter': 307}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:54:21,228] Trial 38 finished with value: 0.8 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0009022754401577258, 'learning_rate': 'constant', 'max_iter': 340}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:54:26,183] Trial 39 finished with value: 0.8125 and parameters: {'hidden_layer_sizes': (150, 100), 'activation': 'logistic', 'solver': 'lbfgs', 'alpha': 0.01456805001140933, 'learning_rate': 'invscaling', 'max_iter': 286}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:54:27,737] Trial 40 finished with value: 0.78125 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0007666839335109092, 'learning_rate': 'invscaling', 'max_iter': 240}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (231) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:54:32,587] Trial 41 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.02919661865953085, 'learning_rate': 'invscaling', 'max_iter': 231}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (223) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:54:37,289] Trial 42 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.06360725041986834, 'learning_rate': 'invscaling', 'max_iter': 223}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:54:42,881] Trial 43 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.03669417120850395, 'learning_rate': 'invscaling', 'max_iter': 269}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (214) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:54:47,379] Trial 44 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.019426474481303763, 'learning_rate': 'invscaling', 'max_iter': 214}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (254) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:54:52,138] Trial 45 finished with value: 0.84375 and parameters: {'hidden_layer_sizes': (100,), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.004558714191837811, 'learning_rate': 'adaptive', 'max_iter': 254}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (238) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (238) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:54:54,670] Trial 46 finished with value: 0.69375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'sgd', 'alpha': 0.001709093569658278, 'learning_rate': 'constant', 'max_iter': 238}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:54:56,929] Trial 47 finished with value: 0.8 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.012074456966764789, 'learning_rate': 'invscaling', 'max_iter': 273}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "[I 2025-01-18 10:54:59,780] Trial 48 finished with value: 0.7875 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'lbfgs', 'alpha': 0.003118631960320202, 'learning_rate': 'invscaling', 'max_iter': 371}. Best is trial 9 with value: 0.84375.\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_25904\\1626584056.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (211) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-01-18 10:55:04,340] Trial 49 finished with value: 0.8375 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.00042339630363977785, 'learning_rate': 'invscaling', 'max_iter': 211}. Best is trial 9 with value: 0.84375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.84375\n",
      "  Params: {'hidden_layer_sizes': (100, 50), 'activation': 'logistic', 'solver': 'adam', 'alpha': 0.014822359495937095, 'learning_rate': 'invscaling', 'max_iter': 211}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每次划分的训练集评分:\n",
      "第1次训练集准确率: 0.9931\n",
      "第1次训练集精确度: 0.9967\n",
      "第1次训练集召回率: 0.9885\n",
      "第1次训练集F1分数: 0.9925\n",
      "第2次训练集准确率: 0.9861\n",
      "第2次训练集精确度: 0.9848\n",
      "第2次训练集召回率: 0.9848\n",
      "第2次训练集F1分数: 0.9848\n",
      "第3次训练集准确率: 0.9653\n",
      "第3次训练集精确度: 0.9588\n",
      "第3次训练集召回率: 0.9662\n",
      "第3次训练集F1分数: 0.9624\n",
      "第4次训练集准确率: 0.6944\n",
      "第4次训练集精确度: 0.2315\n",
      "第4次训练集召回率: 0.3333\n",
      "第4次训练集F1分数: 0.2732\n",
      "第5次训练集准确率: 0.6944\n",
      "第5次训练集精确度: 0.2315\n",
      "第5次训练集召回率: 0.3333\n",
      "第5次训练集F1分数: 0.2732\n",
      "第6次训练集准确率: 0.6944\n",
      "第6次训练集精确度: 0.2315\n",
      "第6次训练集召回率: 0.3333\n",
      "第6次训练集F1分数: 0.2732\n",
      "第7次训练集准确率: 0.6944\n",
      "第7次训练集精确度: 0.2315\n",
      "第7次训练集召回率: 0.3333\n",
      "第7次训练集F1分数: 0.2732\n",
      "第8次训练集准确率: 0.6944\n",
      "第8次训练集精确度: 0.2315\n",
      "第8次训练集召回率: 0.3333\n",
      "第8次训练集F1分数: 0.2732\n",
      "第9次训练集准确率: 0.6944\n",
      "第9次训练集精确度: 0.2315\n",
      "第9次训练集召回率: 0.3333\n",
      "第9次训练集F1分数: 0.2732\n",
      "第10次训练集准确率: 0.6944\n",
      "第10次训练集精确度: 0.2315\n",
      "第10次训练集召回率: 0.3333\n",
      "第10次训练集F1分数: 0.2732\n",
      "\n",
      "每次划分的测试集评分:\n",
      "第1次测试集准确率: 0.7500\n",
      "第1次测试集精确度: 0.3932\n",
      "第1次测试集召回率: 0.4167\n",
      "第1次测试集F1分数: 0.4044\n",
      "第2次测试集准确率: 0.7500\n",
      "第2次测试集精确度: 0.4786\n",
      "第2次测试集召回率: 0.4697\n",
      "第2次测试集F1分数: 0.4683\n",
      "第3次测试集准确率: 0.6875\n",
      "第3次测试集精确度: 0.7333\n",
      "第3次测试集召回率: 0.7424\n",
      "第3次测试集F1分数: 0.7354\n",
      "第4次测试集准确率: 0.6875\n",
      "第4次测试集精确度: 0.2292\n",
      "第4次测试集召回率: 0.3333\n",
      "第4次测试集F1分数: 0.2716\n",
      "第5次测试集准确率: 0.6875\n",
      "第5次测试集精确度: 0.2292\n",
      "第5次测试集召回率: 0.3333\n",
      "第5次测试集F1分数: 0.2716\n",
      "第6次测试集准确率: 0.6875\n",
      "第6次测试集精确度: 0.2292\n",
      "第6次测试集召回率: 0.3333\n",
      "第6次测试集F1分数: 0.2716\n",
      "第7次测试集准确率: 0.6875\n",
      "第7次测试集精确度: 0.2292\n",
      "第7次测试集召回率: 0.3333\n",
      "第7次测试集F1分数: 0.2716\n",
      "第8次测试集准确率: 0.6875\n",
      "第8次测试集精确度: 0.2292\n",
      "第8次测试集召回率: 0.3333\n",
      "第8次测试集F1分数: 0.2716\n",
      "第9次测试集准确率: 0.6875\n",
      "第9次测试集精确度: 0.2292\n",
      "第9次测试集召回率: 0.3333\n",
      "第9次测试集F1分数: 0.2716\n",
      "第10次测试集准确率: 0.6875\n",
      "第10次测试集精确度: 0.2292\n",
      "第10次测试集召回率: 0.3333\n",
      "第10次测试集F1分数: 0.2716\n",
      "\n",
      "平均评分:\n",
      "平均训练集准确率: 0.7806\n",
      "平均训练集精确度: 0.4561\n",
      "平均训练集召回率: 0.5273\n",
      "平均训练集F1分数: 0.4852\n",
      "平均测试集准确率: 0.7000\n",
      "平均测试集精确度: 0.3209\n",
      "平均测试集召回率: 0.3962\n",
      "平均测试集F1分数: 0.3509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\anaconda\\envs\\unimol_tools\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Try on MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "X = fea_df\n",
    "\n",
    "def objective(trial):\n",
    "    # 定义优化的超参数空间\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(100,), (100, 50), (150, 100)])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic'])\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd', 'lbfgs'])\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)  # 正则化参数，取对数分布\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "    max_iter = trial.suggest_int('max_iter', 200, 500)\n",
    "\n",
    "    # 创建 MLP 模型\n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                            activation=activation,\n",
    "                            solver=solver,\n",
    "                            alpha=alpha,\n",
    "                            learning_rate=learning_rate,\n",
    "                            max_iter=max_iter,\n",
    "                            random_state=42)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # 进行分层交叉验证\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 对特征进行标准化处理\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)  # 训练集标准化\n",
    "        X_test_scaled = scaler.transform(X_test)        # 测试集标准化\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # 计算各项评分\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        # 存储评分\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # 计算平均得分，返回优化目标（可以选择不同的得分）\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    return mean_accuracy  # Optuna 会最大化这个目标值\n",
    "\n",
    "study = optuna.create_study(direction='maximize')  # 最大化目标函数\n",
    "study.optimize(objective, n_trials=50)  # 进行50次优化实验\n",
    "\n",
    "# 输出优化结果\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(f\"  Params: {trial.params}\")\n",
    "\n",
    "# 使用最优的超参数训练模型\n",
    "best_model = MLPClassifier(\n",
    "    hidden_layer_sizes=trial.params['hidden_layer_sizes'],\n",
    "    activation=trial.params['activation'],\n",
    "    solver=trial.params['solver'],\n",
    "    alpha=trial.params['alpha'],\n",
    "    learning_rate=trial.params['learning_rate'],\n",
    "    max_iter=trial.params['max_iter'],\n",
    "    random_state=42\n",
    ")\n",
    "# 使用多个评分标准\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "X = fea_df\n",
    "y = label_le\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 执行交叉验证，获取各类评分\n",
    "cv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# 输出每次划分的训练集和测试集评分\n",
    "print(\"每次划分的训练集评分:\")\n",
    "for i in range(10):\n",
    "    print(f\"第{i+1}次训练集准确率: {cv_results['train_accuracy'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次训练集精确度: {cv_results['train_precision_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次训练集召回率: {cv_results['train_recall_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次训练集F1分数: {cv_results['train_f1_macro'][i]:.4f}\")\n",
    "\n",
    "print(\"\\n每次划分的测试集评分:\")\n",
    "for i in range(10):\n",
    "    print(f\"第{i+1}次测试集准确率: {cv_results['test_accuracy'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次测试集精确度: {cv_results['test_precision_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次测试集召回率: {cv_results['test_recall_macro'][i]:.4f}\")\n",
    "    print(f\"第{i+1}次测试集F1分数: {cv_results['test_f1_macro'][i]:.4f}\")\n",
    "\n",
    "# 输出平均评分\n",
    "print(\"\\n平均评分:\")\n",
    "print(f\"平均训练集准确率: {cv_results['train_accuracy'].mean():.4f}\")\n",
    "print(f\"平均训练集精确度: {cv_results['train_precision_macro'].mean():.4f}\")\n",
    "print(f\"平均训练集召回率: {cv_results['train_recall_macro'].mean():.4f}\")\n",
    "print(f\"平均训练集F1分数: {cv_results['train_f1_macro'].mean():.4f}\")\n",
    "\n",
    "print(f\"平均测试集准确率: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"平均测试集精确度: {cv_results['test_precision_macro'].mean():.4f}\")\n",
    "print(f\"平均测试集召回率: {cv_results['test_recall_macro'].mean():.4f}\")\n",
    "print(f\"平均测试集F1分数: {cv_results['test_f1_macro'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分组交叉验证， each catalyst as a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [0.5714285714285714, 0.6571428571428571, 0.8333333333333334, 0.8, 0.6]\n",
      "Mean Accuracy: 0.6924\n",
      "Precisions: [0.19047619047619047, 0.31989247311827956, 0.2777777777777778, 0.26666666666666666, 0.3209876543209877]\n",
      "Mean Precision: 0.2752\n",
      "Recalls: [0.3333333333333333, 0.40444444444444444, 0.3333333333333333, 0.3333333333333333, 0.3666666666666667]\n",
      "Mean Recall: 0.3542\n",
      "F1 Scores: [0.24242424242424243, 0.35714285714285715, 0.30303030303030304, 0.29629629629629634, 0.30885780885780884]\n",
      "Mean F1 Score: 0.3016\n",
      "Confusion Matrices: [array([[ 0,  5,  0],\n",
      "       [ 0, 20,  0],\n",
      "       [ 0, 10,  0]], dtype=int64), array([[ 0,  7,  0],\n",
      "       [ 0, 22,  3],\n",
      "       [ 0,  2,  1]], dtype=int64), array([[ 0,  1,  0],\n",
      "       [ 0, 25,  0],\n",
      "       [ 0,  4,  0]], dtype=int64), array([[ 0,  1,  0],\n",
      "       [ 0, 24,  0],\n",
      "       [ 0,  5,  0]], dtype=int64), array([[ 0,  1,  2],\n",
      "       [ 0, 17,  0],\n",
      "       [ 0,  9,  1]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "X = fea_df  \n",
    "y = label_le\n",
    "groups = np.repeat(np.arange(32), 5)  # 每5条数据是一个组，总共有36组\n",
    "\n",
    "# 创建随机森林模型\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=93,         # 树的数量\n",
    "    max_depth=19,             # 树的最大深度，防止过拟合\n",
    "    min_samples_split=4,      # 内部节点再划分所需的最小样本数\n",
    "    min_samples_leaf=4,       # 叶节点所需的最小样本数\n",
    "    class_weight= {0: 4, 1: 1, 2: 2},  # 处理类别不均衡\n",
    "    max_features='sqrt',\n",
    "    random_state=42)\n",
    "\n",
    "# 创建 GroupKFold 对象\n",
    "gkf = GroupKFold(n_splits=16)\n",
    "\n",
    "# 存储每次测试集的评分\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# 进行分组交叉验证\n",
    "for train_idx, test_idx in gkf.split(X, y, groups=groups):\n",
    "    # print(\"Train indices:\", train_idx)\n",
    "    # print(\"Test indices:\", test_idx)\n",
    "    \n",
    "    # 使用 .iloc 基于整数位置索引划分数据集\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 预测\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 计算各项评分\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "    # 存储评分\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    confusion_matrices.append(confusion)\n",
    "\n",
    "# 输出每次交叉验证的准确率以及其他评分\n",
    "print(f\"Accuracies: {accuracies}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f}\")\n",
    "\n",
    "print(f\"Precisions: {precisions}\")\n",
    "print(f\"Mean Precision: {np.mean(precisions):.4f}\")\n",
    "\n",
    "print(f\"Recalls: {recalls}\")\n",
    "print(f\"Mean Recall: {np.mean(recalls):.4f}\")\n",
    "\n",
    "print(f\"F1 Scores: {f1_scores}\")\n",
    "print(f\"Mean F1 Score: {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "# 输出混淆矩阵（每个折的混淆矩阵）\n",
    "print(f\"Confusion Matrices: {confusion_matrices}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unimol_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
